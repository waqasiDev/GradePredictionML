{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06ac01-c600-480c-9697-0d5b27e23d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 📥 Load dataset\n",
    "df = pd.read_csv(\"../data/processed/updated_data_with_points.csv\")\n",
    "df = df.dropna(subset=[\"Answer\", \"Assigned Points\"])\n",
    "\n",
    "# 🎓 Grading function\n",
    "def score_to_grade(p):\n",
    "    if p >= 90: return \"A\"\n",
    "    elif p >= 80: return \"B\"\n",
    "    elif p >= 70: return \"C\"\n",
    "    elif p >= 60: return \"D\"\n",
    "    else: return \"F\"\n",
    "\n",
    "# 🔀 Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, df[\"Assigned Points\"], test_size=0.2, random_state=42\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "# ⚙️ Device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 🤖 Load CodeBERT\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62bc20e9-1d77-4aa4-8b85-9d7158cec8d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ✨ Generate embeddings\u001b[39;00m\n\u001b[32m     21\u001b[39m X_train_emb = get_embeddings(X_train[\u001b[33m\"\u001b[39m\u001b[33mAnswer\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_test_emb = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     12\u001b[39m         outputs = model(**inputs)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# CLS token: [batch_size, hidden_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     cls_batch = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     16\u001b[39m     embeddings.extend(cls_batch)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(embeddings)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 🧠 Embedding function\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                           padding=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # CLS token: [batch_size, hidden_dim]\n",
    "        cls_batch = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "        embeddings.extend(cls_batch)\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ✨ Generate embeddings\n",
    "X_train_emb = get_embeddings(X_train[\"Answer\"].tolist())\n",
    "X_test_emb = get_embeddings(X_test[\"Answer\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a9af4-af7f-4d9c-9b37-9bf7eb7953ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(embeddings)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ✨ Generate embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m X_train_emb = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m X_test_emb = get_embeddings(X_test[\u001b[33m\"\u001b[39m\u001b[33mAnswer\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 🧠 Train model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m     43\u001b[39m inputs = tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m512\u001b[39m).to(device)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m cls_vector = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :].detach().cpu().numpy().squeeze()\n\u001b[32m     47\u001b[39m embeddings.append(cls_vector)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:914\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    923\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:127\u001b[39m, in \u001b[36mRobertaEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    125\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33mabsolute\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     position_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     embeddings += position_embeddings\n\u001b[32m    129\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/My Mac/Masters in Applied Conputer Science/Thesis/Code/GradePredictionML/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 🧠 Train model\n",
    "regressor = Ridge()\n",
    "regressor.fit(X_train_emb, X_train[\"Assigned Points\"])\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(regressor, \"codebert_ridge_model.pkl\")\n",
    "\n",
    "# 🔮 Predict \n",
    "y_pred = regressor.predict(X_test_emb)\n",
    "X_test[\"Predicted Score\"] = y_pred\n",
    "\n",
    "# 📊 Aggregate per student\n",
    "results_df = X_test.groupby(\"Student_Id\", as_index=False).agg({\n",
    "    \"Predicted Score\": \"sum\",\n",
    "    \"Answer\": \"count\"  # count = number of answers = max score\n",
    "})\n",
    "results_df.rename(columns={\"Answer\": \"Answer Count\"}, inplace=True)\n",
    "results_df[\"Percent Score\"] = (results_df[\"Predicted Score\"] / results_df[\"Answer Count\"]) * 100\n",
    "results_df[\"Predicted Grade\"] = results_df[\"Percent Score\"].apply(score_to_grade)\n",
    "\n",
    "# 🎯 Actual scores per student\n",
    "actual_df = X_test.groupby(\"Student_Id\", as_index=False).agg({\n",
    "    \"Assigned Points\": \"sum\",\n",
    "    \"Answer\": \"count\"\n",
    "})\n",
    "actual_df.rename(columns={\"Answer\": \"Answer Count\"}, inplace=True)\n",
    "actual_df[\"Percent\"] = (actual_df[\"Assigned Points\"] / actual_df[\"Answer Count\"]) * 100\n",
    "actual_df[\"True Grade\"] = actual_df[\"Percent\"].apply(score_to_grade)\n",
    "\n",
    "# 🔁 Merge & report\n",
    "final_df = results_df.merge(actual_df[[\"Student_Id\", \"True Grade\"]], on=\"Student_Id\")\n",
    "print(\"📋 Classification Report (Predicted vs Actual Grades):\")\n",
    "print(classification_report(final_df[\"True Grade\"], final_df[\"Predicted Grade\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
