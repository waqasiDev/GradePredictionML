{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import joblib\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 📥 Load and clean data\n",
    "df = pd.read_csv(\"/content/sample_data/updated_data_with_points.csv\")\n",
    "df = df.dropna(subset=[\"Answer\", \"Assigned Points\"])\n",
    "\n",
    "# 📝 Combine text fields\n",
    "df[\"input_text\"] = (\n",
    "    df[\"Question\"].fillna(\"NO_QUESTION\") + \" [SEP] \" +\n",
    "    df[\"Answer Choices\"].fillna(\"NO_CHOICES\") + \" [SEP] \" +\n",
    "    df[\"Answer\"].fillna(\"NO_ANSWER\")\n",
    ")\n",
    "\n",
    "# 📊 Aggregate by student\n",
    "student_texts = df.groupby(\"Student_Id\")[\"input_text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "student_scores = df.groupby(\"Student_Id\")[\"Assigned Points\"].sum().reset_index()\n",
    "student_scores.columns = [\"Student_Id\", \"Total_Score\"]\n",
    "\n",
    "# 🎓 Convert scores to grades\n",
    "def score_to_grade(p, max_score=25.45):\n",
    "    percent = (p / max_score) * 100\n",
    "    if percent >= 90:\n",
    "        return \"A\"\n",
    "    elif percent >= 80:\n",
    "        return \"B\"\n",
    "    elif percent >= 70:\n",
    "        return \"C\"\n",
    "    elif percent >= 60:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"\n",
    "\n",
    "student_scores[\"Grade\"] = student_scores[\"Total_Score\"].apply(score_to_grade)\n",
    "data = pd.merge(student_texts, student_scores, on=\"Student_Id\")\n",
    "\n",
    "# 🔁 Balance the dataset with duplication and variation\n",
    "def slightly_modify_text(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 5:\n",
    "        random.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def balance_classes(data, label_col=\"Grade\"):\n",
    "    max_count = data[label_col].value_counts().max()\n",
    "    balanced_data = []\n",
    "    for label, group in data.groupby(label_col):\n",
    "        group = group.copy()\n",
    "        while len(group) < max_count:\n",
    "            extra = group.sample(n=min(len(group), max_count - len(group)), replace=True)\n",
    "            extra[\"input_text\"] = extra[\"input_text\"].apply(slightly_modify_text)\n",
    "            group = pd.concat([group, extra], ignore_index=True)\n",
    "        balanced_data.append(group)\n",
    "    return pd.concat(balanced_data, ignore_index=True)\n",
    "\n",
    "data = balance_classes(data, label_col=\"Grade\")\n",
    "\n",
    "# 🔀 Stratified split\n",
    "y = data[\"Grade\"]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "X_train_texts, X_test_texts, y_train_enc, y_test_enc = train_test_split(\n",
    "    data[\"input_text\"], y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 🤖 Load CodeBERT\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# 🔢 Embedding function\n",
    "def get_embeddings(texts, batch_size=8):\n",
    "    all_embeddings = []\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            pooled = outputs.pooler_output.cpu().numpy()\n",
    "            all_embeddings.append(pooled)\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# 💡 Generate embeddings\n",
    "X_train_emb = get_embeddings(X_train_texts.tolist())\n",
    "X_test_emb = get_embeddings(X_test_texts.tolist())\n",
    "\n",
    "# 🚀 Train XGBoost Classifier\n",
    "clf = XGBClassifier(n_estimators=300, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "clf.fit(X_train_emb, y_train_enc)\n",
    "\n",
    "# 💾 Save model and label encoder\n",
    "joblib.dump(clf, \"/content/sample_data/xgboost_grade_model.pkl\")\n",
    "joblib.dump(label_encoder, \"/content/sample_data/label_encoder.pkl\")\n",
    "\n",
    "# 🎯 Evaluate\n",
    "y_pred_enc = clf.predict(X_test_emb)\n",
    "y_test = label_encoder.inverse_transform(y_test_enc)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📉 Confusion Matrix\n",
    "labels = label_encoder.classes_\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (Grades)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
